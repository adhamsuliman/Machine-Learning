{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, SimpleRNN, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) We will read the code in slightly differently than before: \n",
    "dataframe = pd.read_csv(r'C:\\Users\\u353822\\Downloads\\dev-access.csv', engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) We then need to convert to a numpy.ndarray type: \n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"pafzah\",\"password\":\"worldburn432\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Panos1\",\"password\":\"najrijkom\"}}',\n",
       "       ...,\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<script src=\\\\\"http://attacker/malicious\\\\u00e2\\\\u20ac\\\\u2018script.js\\\\\"></script>\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"}}'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data. \n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e) Store all rows and the 0th index as the feature data:\n",
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f) Store all rows and index 1 as the target variable \n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g) In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. \n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character (thus char_level = True)\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i) Need to pad our data as each observation has a different length\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#j) Create your train set to be 75% of the data and your test set to be 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed,Y,test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Model 1 - RNN: The first model will be a pretty minimal RNN with only an embedding layer, LSTM layer, and Dense layer. The next model we will add a few more layers. \n",
    "#a) Start by creating an instance of a Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#b) From there, add an Embedding laye\n",
    "model.add(Embedding(input_dim=num_words, output_dim =32, input_length= max_log_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Add a simple RNN layer\n",
    "model.add(SimpleRNN(32, activation ='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d) Add a Dense Layer:\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e) Compile using the .compile())\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= 'adam' ,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#f) Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 30s 2ms/step - loss: 0.5516 - acc: 0.7230 - val_loss: 0.2404 - val_acc: 0.9253\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 31s 2ms/step - loss: 0.2077 - acc: 0.9338 - val_loss: 0.2543 - val_acc: 0.9251\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 29s 2ms/step - loss: 0.2575 - acc: 0.8845 - val_loss: 0.6588 - val_acc: 0.6181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13606f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#g) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128.\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "6694/6694 [==============================] - 5s 675us/step\n",
      "[0.6597054686908191, 0.6192112339230341]\n"
     ]
    }
   ],
   "source": [
    "#h)  Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again.\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test, batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#3) Model 2 - RNN + Dropout Layers + New Activation Function:\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim =32, input_length= max_log_length))\n",
    "model.add(LSTM(64, recurrent_dropout=.5))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Compile model using the .compile() method:\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= 'adam' ,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#c) Print summary \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 234s 16ms/step - loss: 0.5877 - acc: 0.6819 - val_loss: 0.4517 - val_acc: 0.8876\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 237s 16ms/step - loss: 0.3224 - acc: 0.8960 - val_loss: 0.2221 - val_acc: 0.9351\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 263s 17ms/step - loss: 0.2393 - acc: 0.9389 - val_loss: 0.1773 - val_acc: 0.9544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d9035c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128.\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "6694/6694 [==============================] - 19s 3ms/step\n",
      "[0.17525158247771033, 0.954287421589365]\n"
     ]
    }
   ],
   "source": [
    "#e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again.\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test, batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Recurrent Neural Net Model 3: Build Your Own\n",
    "#a) RNN Requirements: \n",
    "#- Use 5 or more layers\n",
    "#- Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an additional LSTM)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim =64, input_length= max_log_length))\n",
    "model.add(LSTM(4, recurrent_dropout=.5, return_sequences=True))\n",
    "model.add(Dropout(.5))\n",
    "model.add(LSTM(4))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Compiler Requirements: \n",
    "#- Try a new optimizer for the compile step\n",
    "#- Keep accuracy as a metric (feel free to add more metrics if desired)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= 'adadelta' ,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1024, 64)          4032      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1024, 4)           1104      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024, 4)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5,285\n",
      "Trainable params: 5,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#c) Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 150s 10ms/step - loss: 0.9517 - acc: 0.5088 - val_loss: 0.6929 - val_acc: 0.4946\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 144s 10ms/step - loss: 0.6618 - acc: 0.5769 - val_loss: 0.6192 - val_acc: 0.7388\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 149s 10ms/step - loss: 0.4987 - acc: 0.7716 - val_loss: 0.3056 - val_acc: 0.8739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5a505390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128.\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 290s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30824287524155974, 0.8764565342757322]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again.\n",
    "model.evaluate(X_test,y_test,  batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptual Questions: \n",
    "\n",
    "5) Explain the difference between the relu activation function and the sigmoid activation function.\n",
    "The sigmoid function squashes real numbers to a range between 0 and 1. The RelU activation function is zero below 0 and linar with a slope of 1 for x values larger than 0.\n",
    "\n",
    "6) Describe what one epoch actually is (epoch was a parameter used in the .fit() method).\n",
    "One epoch means that the model is trained with the entire data set once. If you train the model with 2 epochs, the model will be trained with the training data twice. \n",
    "\n",
    "7) Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets.\n",
    "Dropout is where nodes within the neural network are randomly dropped in order to acheive less  paramaters for the overall model.\n",
    "\n",
    "8) Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?\n",
    "CNNs are better for this type of problem because the data is static. If the data was time sensitive, an RNN would be a better alternative. CNNs outperform RNNs for visual image recognition, and they also do better on static data.\n",
    "\n",
    "9) Explain what RNN problem is solved using LSTM and briefly describe how.\n",
    "LSTMs are used for time senitive data. LSTM use nodes connected to one another in a cyclical fashion. The bottom layer is able to store history because they are connected to themselves in a recurrent fashion. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
